{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import numpy as np\n",
    "import glob, os\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dir = './models/*'\n",
    "model_files = glob.glob(dir)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "        self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
    "        \n",
    "        self.rnn1 = nn.LSTM(\n",
    "          input_size=n_features,\n",
    "          hidden_size=self.hidden_dim,\n",
    "          num_layers=1,\n",
    "          batch_first=True\n",
    "        )\n",
    "        self.rnn2 = nn.LSTM(\n",
    "          input_size=self.hidden_dim,\n",
    "          hidden_size=embedding_dim,\n",
    "          num_layers=1,\n",
    "          batch_first=True\n",
    "        )\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.reshape((1, self.seq_len, self.n_features))\n",
    "        \n",
    "        x, (_, _) = self.rnn1(x)\n",
    "        x, (hidden_n, _) = self.rnn2(x)\n",
    "        \n",
    "        return hidden_n.reshape((self.seq_len, self.embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_dim=64, n_features=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "        self.hidden_dim, self.n_features = 2 * input_dim, n_features\n",
    "       \n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=input_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.rnn2 = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "    def forward(self, x):\n",
    "        x = x.repeat(self.seq_len, self.n_features)\n",
    "        x = x.reshape((self.n_features, self.seq_len, self.input_dim))\n",
    "        \n",
    "        x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "        x, (hidden_n, cell_n) = self.rnn2(x)\n",
    "        x = x.reshape((self.n_features, self.hidden_dim))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "        super(RecurrentAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
    "        self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/model_onehot_x1data_MSE_epoch200.pth'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = torch.load(model_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecurrentAutoencoder(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(265, 256, batch_first=True)\n",
      "    (rnn2): LSTM(256, 128, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(128, 128, batch_first=True)\n",
      "    (rnn2): LSTM(128, 256, batch_first=True)\n",
      "    (output_layer): Linear(in_features=256, out_features=265, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_LSTM)\n",
    "alpha = list(string.ascii_uppercase)\n",
    "chr2index = {alpha[i]:i for i in range(len(alpha))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chr2OH(alphabet):\n",
    "    oh = [0 for i in range(len(alpha))]\n",
    "    index = chr2index[alphabet]\n",
    "    oh[index] = 1\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = './datasets/fsm/seq/0graph*.txt'\n",
    "# file read\n",
    "all_names = []\n",
    "all_data = []\n",
    "sequence_length = []\n",
    "alpha = list(string.ascii_uppercase)\n",
    "data_length = len(glob.glob(dir))\n",
    "\n",
    "files = glob.glob(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(dir)\n",
    "for file in files:\n",
    "    datasets = []\n",
    "    for rf in open(file, 'r'):\n",
    "        (u, v, w) = rf[1:-2].split(', ')\n",
    "        datasets.append(chr2OH(u[1]) + chr2OH(v[1]) +[float(w)])\n",
    "        #datasets.append([chr2index[u[1]], chr2index[v[1]], float(w)])\n",
    "    sequence_length.append(len(datasets))\n",
    "    all_data.append(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 5\n",
    "#zeros = np.zeros(53)\n",
    "zeros = [0 for i in range(53)]\n",
    "for ind, data in enumerate(all_data):\n",
    "    if len(data) != max_sequence_length:\n",
    "        for i in range(len(data), max_sequence_length):\n",
    "            all_data[ind].append(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = np.array([np.array(arr) for arr in all_data])\n",
    "all_d = []\n",
    "for ind, data in enumerate(all_data):\n",
    "    all_d.append(data.flatten())\n",
    "all_d\n",
    "len(all_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset_1(nparrays):\n",
    "    dataset = [torch.tensor(s).unsqueeze(0).float() for s in nparrays]\n",
    "  #step_per_epoch, seq_len, llen, n_features = torch.stack(dataset).shape\n",
    "    #n_seq, m ,seq_len, n_features = print(torch.stack(dataset).shape)\n",
    "    print(torch.stack(dataset).shape)\n",
    "    n_seq, seq_len, n_features = torch.stack(dataset).shape\n",
    "    #print( n_seq, m , seq_len, n_features )\n",
    "    return dataset, seq_len, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([382, 1, 265])\n"
     ]
    }
   ],
   "source": [
    "ts, seq_len_1d , n_features_1d  = create_dataset_1(all_d)\n",
    "ss  = [torch.tensor(s).unsqueeze(0).float() for s in all_d]\n",
    "T = ts[1]\n",
    "F = ts[0]\n",
    "M = ts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, 10.5201,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  2.0965,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.5716,  0.0000,\n",
       "          0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  7.4006,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          2.5041]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset):\n",
    "    predictions, losses = [], []\n",
    "    criterion = nn.L1Loss(reduction='mean').to(device)\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        for seq_true in dataset:\n",
    "            seq_true = seq_true.to(device)\n",
    "            seq_pred = model(seq_true)\n",
    "            print(seq_pred.shape)\n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "            print(loss)\n",
    "            print(seq_pred.cpu().numpy().flatten())\n",
    "            predictions.append(seq_pred.cpu().numpy().flatten())\n",
    "            losses.append(loss.item())\n",
    "    return predictions, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([265, 265])\n",
      "tensor(0.0180, device='cuda:0')\n",
      "[ 5.2082576e-02 -4.7331393e-02  1.4518905e-02 ...  3.3811826e-02\n",
      " -4.0389672e-03  4.6819334e+00]\n",
      "[array([ 5.2082576e-02, -4.7331393e-02,  1.4518905e-02, ...,\n",
      "        3.3811826e-02, -4.0389672e-03,  4.6819334e+00], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "prT, _ = predict(model_onehot_L1, T)\n",
    "\n",
    "print(prT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([265, 265])\n",
      "tensor(0.0199, device='cuda:0')\n",
      "[ 0.50145906  0.46425033 -0.00424828 ...  0.02180772 -0.0035957\n",
      "  2.4830134 ]\n",
      "[array([ 0.50145906,  0.46425033, -0.00424828, ...,  0.02180772,\n",
      "       -0.0035957 ,  2.4830134 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "prF, _ = predict(model_onehot_L1, F)\n",
    "print(prF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([265, 265])\n",
      "tensor(0.0196, device='cuda:0')\n",
      "[ 0.502443    0.4656234  -0.00696728 ...  0.02163182 -0.00367077\n",
      "  2.482694  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.502443  ,  0.4656234 , -0.00696728, ...,  0.02163182,\n",
       "        -0.00367077,  2.482694  ], dtype=float32)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prM,_= predict(model_onehot_L1, M)\n",
    "prM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[169.08931]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(prT, prF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[169.93518]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(prT, prM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5390404]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(prF, prM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74176836]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(prT, prF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7395986]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(prT, prM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99998033]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(prF, prM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
